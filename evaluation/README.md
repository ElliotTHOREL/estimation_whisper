# ğŸ“Š SystÃ¨me d'Ã‰valuation des ModÃ¨les Whisper

Ce dossier contient un systÃ¨me complet d'Ã©valuation des modÃ¨les Whisper utilisant l'API pour des comparaisons de performance et de qualitÃ©.

## ğŸ“ Structure

```
evaluation/
â”œâ”€â”€ code/                           # Scripts d'Ã©valuation
â”‚   â”œâ”€â”€ evaluate_whisper_models.py  # Script principal d'Ã©valuation
â”‚   â”œâ”€â”€ generate_plots.py           # GÃ©nÃ©ration des graphiques
â”‚   â”œâ”€â”€ detailed_error_analysis.py  # Analyse dÃ©taillÃ©e des erreurs
â”‚   â””â”€â”€ run_full_evaluation.py      # Script complet automatisÃ©
â”œâ”€â”€ benchmarks/                     # RÃ©sultats et graphiques
â”‚   â”œâ”€â”€ evaluation_results_*.json   # RÃ©sultats JSON
â”‚   â”œâ”€â”€ detailed_results_*.json     # RÃ©sultats dÃ©taillÃ©s
â”‚   â”œâ”€â”€ metrics_summary_*.csv       # MÃ©triques au format CSV
â”‚   â”œâ”€â”€ *.png                       # Graphiques gÃ©nÃ©rÃ©s
â”‚   â””â”€â”€ evaluation_report.html      # Rapport HTML
â””â”€â”€ README.md                       # Ce fichier
```

## ğŸš€ Utilisation Rapide

### PrÃ©requis
1. **API Whisper** doit Ãªtre lancÃ©e :
   ```bash
   source whisper_env/bin/activate
   python api/start_api.py --host 0.0.0.0 --port 8000
   ```

2. **ModÃ¨les chargÃ©s** dans l'API (recommandÃ© pour des rÃ©sultats rapides)

### Ã‰valuation complÃ¨te automatisÃ©e
```bash
# Ã‰valuation sur 30 Ã©chantillons (recommandÃ©)
python evaluation/code/run_full_evaluation.py --samples 30

# Ã‰valuation rapide sur 10 Ã©chantillons
python evaluation/code/run_full_evaluation.py --samples 10

# RegÃ©nÃ©rer seulement les graphiques
python evaluation/code/run_full_evaluation.py --skip-evaluation
```

## ğŸ”§ Scripts Individuels

### 1. Ã‰valuation des ModÃ¨les
```bash
# Script d'Ã©valuation principal
python evaluation/code/evaluate_whisper_models.py
```

**Ce qu'il fait :**
- Utilise l'API Whisper pour transcrire des Ã©chantillons audio
- Calcule les mÃ©triques WER, CER, BLEU, MER
- Mesure les temps d'infÃ©rence
- Sauvegarde les rÃ©sultats en JSON et CSV

### 2. GÃ©nÃ©ration des Graphiques
```bash
# GÃ©nÃ¨re tous les graphiques
python evaluation/code/generate_plots.py

# Utiliser un fichier de rÃ©sultats spÃ©cifique
python evaluation/code/generate_plots.py --results-file evaluation/benchmarks/evaluation_results_20250623_152800.json
```

**Graphiques gÃ©nÃ©rÃ©s :**
- **Comparaison des mÃ©triques** : Barres comparatives pour WER, CER, BLEU, MER
- **Performance vs PrÃ©cision** : Scatter plots vitesse vs qualitÃ©
- **Radar Chart** : Vue globale de tous les modÃ¨les
- **Tableau rÃ©capitulatif** : RÃ©sumÃ© formatÃ©

### 3. Analyse DÃ©taillÃ©e des Erreurs
```bash
# Analyse dÃ©taillÃ©e des erreurs (substitutions, insertions, suppressions)
python evaluation/code/detailed_error_analysis.py

# Utiliser un fichier de rÃ©sultats dÃ©taillÃ©s spÃ©cifique
python evaluation/code/detailed_error_analysis.py --results-file evaluation/benchmarks/detailed_results_20250623_152800.json
```

**Analyses gÃ©nÃ©rÃ©es :**
- **Rapports dÃ©taillÃ©s par modÃ¨le** : WER par clip avec dÃ©composition des erreurs
- **Graphiques de distribution des erreurs** : RÃ©partition par type d'erreur
- **Histogrammes WER** : Distribution des performances par clip
- **Exemples des pires erreurs** : Cas d'Ã©chec avec analyse

## ğŸ“ˆ MÃ©triques CalculÃ©es

| MÃ©trique | Description | InterprÃ©tation |
|----------|-------------|----------------|
| **WER** | Word Error Rate | Plus bas = meilleur |
| **CER** | Character Error Rate | Plus bas = meilleur |
| **BLEU** | BiLingual Evaluation Understudy | Plus haut = meilleur |
| **MER** | Match Error Rate | Plus bas = meilleur |
| **Temps d'infÃ©rence** | Temps moyen par audio | Plus bas = plus rapide |
| **Taux de succÃ¨s** | % de transcriptions rÃ©ussies | Plus haut = plus fiable |

## ğŸ¯ Exemples d'Utilisation

### Ã‰valuation Standard
```bash
# Ã‰valuation complÃ¨te sur 30 Ã©chantillons
python evaluation/code/run_full_evaluation.py --samples 30
```

### Ã‰valuation Rapide pour Tests
```bash
# Test rapide sur 5 Ã©chantillons
python evaluation/code/run_full_evaluation.py --samples 5
```

### Analyse d'Anciens RÃ©sultats
```bash
# RegÃ©nÃ©rer graphiques Ã  partir de rÃ©sultats existants
python evaluation/code/generate_plots.py --results-file evaluation/benchmarks/evaluation_results_20250623_152800.json
```

## ğŸ“Š InterprÃ©tation des RÃ©sultats

### Graphique Comparaison des MÃ©triques
- **WER/CER/MER** : Plus la barre est basse, meilleur est le modÃ¨le
- **BLEU** : Plus la barre est haute, meilleur est le modÃ¨le

### Graphique Performance vs PrÃ©cision
- **Coin bas-gauche** : Rapide ET prÃ©cis (idÃ©al)
- **Coin bas-droite** : Lent mais prÃ©cis
- **Coin haut-gauche** : Rapide mais imprÃ©cis

### Radar Chart
- **PÃ©rimÃ¨tre large** : ModÃ¨le polyvalent
- **Forme Ã©quilibrÃ©e** : Performance consistante
- **Pics** : Points forts spÃ©cifiques

## ğŸ”§ Configuration

### Modification du Nombre d'Ã‰chantillons
Ã‰ditez `evaluation/code/evaluate_whisper_models.py` ligne ~202 :
```python
results = evaluator.run_evaluation(num_samples=50)  # Modifier ici
```

### Ajout de Nouveaux ModÃ¨les
Ã‰ditez `evaluation/code/evaluate_whisper_models.py` ligne ~43 :
```python
self.models = ["tiny", "base", "small", "medium", "large", "large-v3", "nouveau_modele"]
```

### Personnalisation des Graphiques
Modifiez `evaluation/code/generate_plots.py` :
- Couleurs : ligne ~38
- Tailles : ligne ~36
- MÃ©triques : lignes ~58, ~168

## ğŸ“ Fichiers de Sortie

### RÃ©sultats JSON
```json
{
  "tiny": {
    "wer": 0.245,
    "cer": 0.123,
    "bleu": 0.678,
    "mer": 0.189,
    "avg_inference_time": 0.45,
    "success_rate": 98.5
  }
}
```

### CSV MÃ©triques
| Model | WER | CER | BLEU | MER | Temps | SuccÃ¨s |
|-------|-----|-----|------|-----|-------|--------|
| tiny  | 0.245 | 0.123 | 0.678 | 0.189 | 0.45s | 98.5% |

## ğŸš¨ DÃ©pannage

### API Non Accessible
```bash
# VÃ©rifier l'Ã©tat de l'API
curl http://localhost:8000/health

# RedÃ©marrer l'API
source whisper_env/bin/activate
python api/start_api.py --host 0.0.0.0 --port 8000
```

### Erreurs de MÃ©moire
- RÃ©duire le nombre d'Ã©chantillons avec `--samples 10`
- Vider le cache API : `curl -X POST http://localhost:8000/cache/clear`

### Graphiques Non GÃ©nÃ©rÃ©s
```bash
# VÃ©rifier les dÃ©pendances
pip install matplotlib seaborn pandas

# Test de gÃ©nÃ©ration manuelle
python evaluation/code/generate_plots.py
```

## ğŸ“ Support

Pour des questions spÃ©cifiques :
1. Consultez les logs dÃ©taillÃ©s des scripts
2. VÃ©rifiez que l'API fonctionne correctement
3. Validez que le dataset Common Voice est accessible

---

## ğŸ‰ RÃ©sultats Attendus

AprÃ¨s une Ã©valuation complÃ¨te, vous obtiendrez :
- âœ… Comparaison objective de tous les modÃ¨les
- âœ… Graphiques publication-ready
- âœ… Rapport HTML navigable
- âœ… DonnÃ©es exportables (CSV, JSON)
- âœ… MÃ©triques de performance dÃ©taillÃ©es 