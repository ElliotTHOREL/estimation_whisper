#!/usr/bin/env python3
"""
Gestionnaire de ressources pour l'√©valuation optimis√©e des mod√®les

Ce script :
1. V√©rifie les ressources disponibles (RAM/GPU)
2. Recommande une strat√©gie d'√©valuation optimale
3. Lance automatiquement l'√©valuation par batch
4. Surveille les ressources pendant l'ex√©cution

Usage:
    python evaluation/code/resource_manager.py --auto --samples 30
"""

import psutil
import subprocess
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ResourceManager:
    """Gestionnaire de ressources pour l'√©valuation des mod√®les"""
    
    def __init__(self):
        self.memory_info = self.get_system_info()
        self.model_memory_estimates = self.get_model_memory_estimates()
        
    def get_system_info(self) -> Dict:
        """R√©cup√®re les informations syst√®me"""
        memory = psutil.virtual_memory()
        
        info = {
            "total_ram_gb": round(memory.total / (1024**3), 2),
            "available_ram_gb": round(memory.available / (1024**3), 2),
            "cpu_count": psutil.cpu_count(),
            "gpu_available": False
        }
        
        # V√©rification GPU
        try:
            import torch
            if torch.cuda.is_available():
                gpu_props = torch.cuda.get_device_properties(0)
                info.update({
                    "gpu_available": True,
                    "gpu_name": gpu_props.name,
                    "gpu_memory_gb": round(gpu_props.total_memory / (1024**3), 2)
                })
        except ImportError:
            pass
            
        return info
    
    def get_model_memory_estimates(self) -> Dict[str, float]:
        """Estimations de consommation RAM par mod√®le (en GB)"""
        return {
            "whisper:tiny": 1.0,
            "whisper:base": 1.0,
            "whisper:small": 2.0,
            "whisper:medium": 5.0,
            "whisper:large": 10.0,
            "whisper:large-v3": 10.0,
            "wav2vec2:xlsr-53-french": 3.0,
            "wav2vec2:bhuang-french": 2.5,
            "wav2vec2:jonatasgrosman-xlsr": 3.5,
            "wav2vec2:jonatasgrosman-voxpopuli": 3.0,
            "wav2vec2:wasertech-cv9": 2.5,
            "seamless:medium": 6.0,
            "seamless:large": 12.0,
            "gemma3n:e2b": 8.0,
            "gemma3n:e4b": 15.0,
        }
    
    def analyze_resources(self) -> Dict:
        """Analyse les ressources et recommande une strat√©gie"""
        analysis = {
            "total_ram": self.memory_info["total_ram_gb"],
            "available_ram": self.memory_info["available_ram_gb"],
            "safe_ram_limit": self.memory_info["available_ram_gb"] * 0.7,  # 70% de la RAM dispo
            "recommendations": []
        }
        
        # Classement des mod√®les par consommation
        models_by_memory = sorted(
            self.model_memory_estimates.items(),
            key=lambda x: x[1]
        )
        
        # Strat√©gie recommand√©e
        if analysis["safe_ram_limit"] > 15:
            strategy = "aggressive"
            batch_size = 4
            analysis["recommendations"].append("RAM suffisante pour des batches larges")
        elif analysis["safe_ram_limit"] > 8:
            strategy = "moderate"
            batch_size = 3
            analysis["recommendations"].append("RAM mod√©r√©e - batches moyens recommand√©s")
        else:
            strategy = "conservative"
            batch_size = 2
            analysis["recommendations"].append("RAM limit√©e - petits batches obligatoires")
        
        analysis.update({
            "strategy": strategy,
            "recommended_batch_size": batch_size,
            "estimated_total_time_minutes": len(self.model_memory_estimates) * 5,  # 5min par mod√®le estim√©
        })
        
        return analysis
    
    def create_optimized_batches(self, analysis: Dict) -> Dict[str, List[Tuple[str, str]]]:
        """Cr√©e des batches optimis√©s selon l'analyse"""
        safe_limit = analysis["safe_ram_limit"]
        
        # Mod√®les tri√©s par consommation RAM
        models_sorted = sorted(
            [(k.split(":")[0], k.split(":")[1], v) for k, v in self.model_memory_estimates.items()],
            key=lambda x: x[2]
        )
        
        batches = {}
        current_batch = []
        current_memory = 0
        batch_num = 1
        
        for model_type, model_size, memory_need in models_sorted:
            # Si ajouter ce mod√®le d√©passe la limite, cr√©er un nouveau batch
            if current_memory + memory_need > safe_limit and current_batch:
                batches[f"batch_{batch_num}"] = current_batch.copy()
                current_batch = []
                current_memory = 0
                batch_num += 1
            
            current_batch.append((model_type, model_size))
            current_memory += memory_need
        
        # Ajouter le dernier batch s'il n'est pas vide
        if current_batch:
            batches[f"batch_{batch_num}"] = current_batch
        
        return batches
    
    def display_analysis(self, analysis: Dict, batches: Dict):
        """Affiche l'analyse des ressources"""
        logger.info("\n" + "="*60)
        logger.info("üîç ANALYSE DES RESSOURCES SYST√àME")
        logger.info("="*60)
        logger.info(f"üíæ RAM totale: {analysis['total_ram']:.1f} GB")
        logger.info(f"üíæ RAM disponible: {analysis['available_ram']:.1f} GB")
        logger.info(f"üéØ Limite s√©curis√©e: {analysis['safe_ram_limit']:.1f} GB")
        
        if self.memory_info["gpu_available"]:
            logger.info(f"üéÆ GPU: {self.memory_info['gpu_name']} ({self.memory_info['gpu_memory_gb']:.1f} GB)")
        
        logger.info(f"\nüìã Strat√©gie recommand√©e: {analysis['strategy'].upper()}")
        logger.info(f"‚è±Ô∏è Temps estim√©: {analysis['estimated_total_time_minutes']} minutes")
        
        logger.info(f"\nüì¶ {len(batches)} batches optimis√©s:")
        for batch_name, models in batches.items():
            total_memory = sum(self.model_memory_estimates[f"{t}:{s}"] for t, s in models)
            logger.info(f"  {batch_name}: {len(models)} mod√®les (~{total_memory:.1f} GB)")
            for model_type, model_size in models:
                memory = self.model_memory_estimates[f"{model_type}:{model_size}"]
                logger.info(f"    - {model_type}:{model_size} (~{memory:.1f} GB)")
        
        logger.info("\nüí° Recommandations:")
        for rec in analysis["recommendations"]:
            logger.info(f"  ‚Ä¢ {rec}")
    
    def run_evaluation_with_monitoring(self, batches: Dict, num_samples: int = 30):
        """Lance l'√©valuation avec monitoring automatique"""
        logger.info("\nüöÄ LANCEMENT DE L'√âVALUATION AVEC MONITORING")
        
        # D√©marrage du monitoring en arri√®re-plan
        monitor_cmd = [
            sys.executable, "memory_monitor.py",
            "--threshold", "80",
            "--log-interval", "30"
        ]
        
        logger.info("üîç D√©marrage du monitoring m√©moire...")
        monitor_process = subprocess.Popen(
            monitor_cmd,
            cwd=Path(__file__).parent,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        try:
            # Lancement de l'√©valuation par batch
            batch_names = list(batches.keys())
            eval_cmd = [
                sys.executable, "batch_evaluation.py",
                "--samples", str(num_samples),
                "--batches"
            ] + batch_names
            
            logger.info("üìä D√©marrage de l'√©valuation par batch...")
            eval_result = subprocess.run(
                eval_cmd,
                cwd=Path(__file__).parent,
                capture_output=True,
                text=True,
                timeout=7200  # 2h timeout
            )
            
            if eval_result.returncode == 0:
                logger.info("‚úÖ √âvaluation termin√©e avec succ√®s!")
                print(eval_result.stdout)
            else:
                logger.error("‚ùå Erreur lors de l'√©valuation:")
                print(eval_result.stderr)
                
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Timeout de l'√©valuation (2h d√©pass√©es)")
        
        finally:
            # Arr√™t du monitoring
            logger.info("üõë Arr√™t du monitoring...")
            monitor_process.terminate()
            monitor_process.wait()
    
    def interactive_mode(self):
        """Mode interactif pour choisir la strat√©gie"""
        analysis = self.analyze_resources()
        batches = self.create_optimized_batches(analysis)
        
        self.display_analysis(analysis, batches)
        
        print("\n" + "="*60)
        print("üéØ OPTIONS D'√âVALUATION")
        print("="*60)
        print("1. √âvaluation automatique compl√®te (recommand√©)")
        print("2. √âvaluation batch par batch manuelle")
        print("3. S√©lection de batches sp√©cifiques")
        print("4. Test rapide (5 √©chantillons)")
        print("5. Analyse seulement (pas d'√©valuation)")
        
        while True:
            try:
                choice = input("\nChoisissez une option (1-5): ").strip()
                
                if choice == "1":
                    samples = int(input("Nombre d'√©chantillons par batch (d√©faut: 30): ") or "30")
                    self.run_evaluation_with_monitoring(batches, samples)
                    break
                elif choice == "2":
                    self.manual_batch_mode(batches)
                    break
                elif choice == "3":
                    self.selective_batch_mode(batches)
                    break
                elif choice == "4":
                    self.run_evaluation_with_monitoring(batches, 5)
                    break
                elif choice == "5":
                    logger.info("üìä Analyse termin√©e - aucune √©valuation lanc√©e")
                    break
                else:
                    print("‚ùå Option invalide, veuillez choisir entre 1 et 5")
                    
            except (ValueError, KeyboardInterrupt):
                print("\nüõë Op√©ration annul√©e")
                break
    
    def manual_batch_mode(self, batches: Dict):
        """Mode manuel batch par batch"""
        logger.info("üîß Mode manuel s√©lectionn√©")
        for batch_name in batches.keys():
            response = input(f"\nEx√©cuter {batch_name}? (o/n): ").strip().lower()
            if response in ['o', 'oui', 'y', 'yes']:
                subprocess.run([
                    sys.executable, "batch_evaluation.py",
                    "--batches", batch_name,
                    "--samples", "30"
                ], cwd=Path(__file__).parent)
    
    def selective_batch_mode(self, batches: Dict):
        """Mode de s√©lection de batches sp√©cifiques"""
        print("\nüì¶ Batches disponibles:")
        for i, batch_name in enumerate(batches.keys(), 1):
            print(f"  {i}. {batch_name}")
        
        selected = input("\nNum√©ros des batches √† ex√©cuter (ex: 1,3,5): ").strip()
        try:
            indices = [int(x.strip()) - 1 for x in selected.split(",")]
            selected_batches = [list(batches.keys())[i] for i in indices]
            
            subprocess.run([
                sys.executable, "batch_evaluation.py",
                "--batches"
            ] + selected_batches + [
                "--samples", "30"
            ], cwd=Path(__file__).parent)
            
        except (ValueError, IndexError):
            logger.error("‚ùå S√©lection invalide")

def main():
    parser = argparse.ArgumentParser(description="Gestionnaire de ressources pour l'√©valuation")
    parser.add_argument('--auto', action='store_true',
                       help="Mode automatique sans interaction")
    parser.add_argument('--samples', type=int, default=30,
                       help="Nombre d'√©chantillons par batch")
    parser.add_argument('--analyze-only', action='store_true',
                       help="Analyse seulement, pas d'√©valuation")
    
    args = parser.parse_args()
    
    manager = ResourceManager()
    
    if args.analyze_only:
        analysis = manager.analyze_resources()
        batches = manager.create_optimized_batches(analysis)
        manager.display_analysis(analysis, batches)
    elif args.auto:
        analysis = manager.analyze_resources()
        batches = manager.create_optimized_batches(analysis)
        manager.display_analysis(analysis, batches)
        manager.run_evaluation_with_monitoring(batches, args.samples)
    else:
        manager.interactive_mode()

if __name__ == "__main__":
    main() 