# ğŸ™ï¸ API Whisper - ModÃ¨les de Transcription

Cette API fournit un accÃ¨s aux modÃ¨les Whisper d'OpenAI pour la transcription audio avec mise en cache intelligente.

## ğŸ—ï¸ Architecture

**L'API contient UNIQUEMENT les modÃ¨les Whisper** - les Ã©valuations se font Ã  l'extÃ©rieur en appelant cette API.

```
ğŸ“ Projet/
â”œâ”€â”€ ğŸ“ api/                    # âš¡ API des modÃ¨les (SEULEMENT)
â”‚   â”œâ”€â”€ whisper_api.py         # Serveur FastAPI avec cache
â”‚   â”œâ”€â”€ start_api.py           # Script de dÃ©marrage
â”‚   â””â”€â”€ README.md              # Documentation API
â”œâ”€â”€ evaluate_with_api.py       # ğŸ”¬ Ã‰valuations externes
â”œâ”€â”€ evaluation_with_api.py     # Fonctions d'Ã©valuation
â”œâ”€â”€ whisper_client.py          # Client pour appeler l'API
â”œâ”€â”€ evaluation/code/metrics.py # Calcul des mÃ©triques
â””â”€â”€ plot_eval_results.py       # GÃ©nÃ©ration des graphiques
```

## ğŸš€ DÃ©marrage rapide

### 1. DÃ©marrer l'API

```bash
# MÃ©thode 1: Script de dÃ©marrage
cd api
python start_api.py --host 0.0.0.0 --port 8000

# MÃ©thode 2: Directement
python api/whisper_api.py --host 0.0.0.0 --port 8000 --reload
```

### 2. Tester l'API

```bash
# VÃ©rifier la santÃ©
curl http://localhost:8000/health

# Lister les modÃ¨les disponibles
curl http://localhost:8000/models
```

### 3. Faire une Ã©valuation (depuis la racine)

```bash
# Ã‰valuation complÃ¨te avec dÃ©marrage automatique de l'API
python evaluate_with_api.py --clips 21-25

# Avec API dÃ©jÃ  dÃ©marrÃ©e
python evaluate_with_api.py --clips 1-10 --api-url http://localhost:8000
```

## ğŸ“¡ Endpoints de l'API

### ğŸ¯ Transcription

#### `POST /transcribe`
Transcrit un fichier audio uploadÃ©.

```bash
curl -X POST "http://localhost:8000/transcribe" \
  -F "audio_file=@audio.mp3" \
  -F "model_size=base" \
  -F "language=fr"
```

#### `POST /transcribe_file`  
Transcrit un fichier via son chemin sur le serveur.

```bash
curl -X POST "http://localhost:8000/transcribe_file" \
  -F "file_path=/path/to/audio.mp3" \
  -F "model_size=large-v3" \
  -F "language=fr"
```

### ğŸ“Š Gestion

#### `GET /models`
Liste des modÃ¨les disponibles.

```json
{
  "available_models": ["tiny", "base", "small", "medium", "large", "large-v3"],
  "model_details": {
    "tiny": {"size": "~39 MB", "speed": "~32x"},
    "base": {"size": "~74 MB", "speed": "~16x"},
    "small": {"size": "~244 MB", "speed": "~6x"},
    "medium": {"size": "~769 MB", "speed": "~2x"},
    "large": {"size": "~1550 MB", "speed": "~1x"},
    "large-v3": {"size": "~1550 MB", "speed": "~1x"}
  }
}
```

#### `GET /cache/info`
Informations sur le cache des modÃ¨les.

#### `POST /cache/clear`
Vide le cache des modÃ¨les.

#### `GET /health`
Ã‰tat de santÃ© de l'API.

## âš¡ Mise en cache intelligente

L'API maintient un cache LRU (Least Recently Used) des modÃ¨les :
- **Maximum 3 modÃ¨les** en mÃ©moire simultanÃ©ment
- **Ã‰viction automatique** des modÃ¨les les moins utilisÃ©s
- **AccÃ¨s rapide** aux modÃ¨les populaires

## ğŸ”§ Configuration

### Variables d'environnement

```bash
export LANGUE=fr                    # Langue par dÃ©faut
export WHISPER_CACHE_DIR=/tmp/cache # Dossier cache (optionnel)
```

### ParamÃ¨tres de dÃ©marrage

```bash
python api/start_api.py \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 4 \
  --reload
```

## ğŸ“ˆ Utilisation pour les Ã©valuations

L'API est conÃ§ue pour Ãªtre appelÃ©e par des scripts d'Ã©valuation externes :

1. **DÃ©marrage** : L'API dÃ©marre et charge les modÃ¨les Ã  la demande
2. **Ã‰valuation** : Les scripts externes appellent l'API pour chaque transcription
3. **Cache** : Les modÃ¨les restent en mÃ©moire pour les appels suivants
4. **MÃ©triques** : CalculÃ©es cÃ´tÃ© client, pas dans l'API

### Exemple d'usage programmatique

```python
from whisper_client import WhisperAPIClient

# Connexion Ã  l'API
client = WhisperAPIClient("http://localhost:8000")

# Transcription
result = client.transcribe_file(
    file_path="audio.mp3",
    model_size="large-v3",
    language="fr"
)

print(result["text"])
print(f"Temps d'infÃ©rence: {result['inference_time']:.2f}s")
```

## ğŸ“¦ DÃ©pendances

```bash
# Installation (dans l'environnement virtuel)
pip install fastapi uvicorn transformers torch librosa
```

## ğŸ› Debugging

### Logs de l'API
```bash
# DÃ©marrage avec logs dÃ©taillÃ©s
python api/start_api.py --log-level debug
```

### VÃ©rification du cache
```bash
curl http://localhost:8000/cache/info
```

### Test de connectivitÃ©
```bash
curl http://localhost:8000/health
```

## ğŸš¨ Notes importantes

- **CPU vs GPU** : L'API dÃ©tecte automatiquement CUDA
- **MÃ©moire** : Surveiller l'usage mÃ©moire avec plusieurs modÃ¨les
- **Concurrence** : L'API gÃ¨re les requÃªtes simultanÃ©es
- **SÃ©curitÃ©** : En production, configurer HTTPS et authentification 