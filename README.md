# ğŸ™ï¸ SystÃ¨me d'Ã‰valuation Whisper avec API

SystÃ¨me complet d'Ã©valuation des modÃ¨les Whisper d'OpenAI avec architecture microservices sÃ©parÃ©e.

## ğŸ—ï¸ Architecture

**Principe clÃ©** : SÃ©paration complÃ¨te entre les modÃ¨les et les Ã©valuations

```
ğŸ“ Projet Whisper/
â”œâ”€â”€ ğŸ”§ API (modÃ¨les uniquement)
â”‚   â”œâ”€â”€ whisper_api.py      # Serveur FastAPI avec cache
â”‚   â”œâ”€â”€ start_api.py        # Script de dÃ©marrage
â”‚   â””â”€â”€ README.md           # Documentation API
â”œâ”€â”€ ğŸ”¬ Ã‰valuations (externes)
â”‚   â”œâ”€â”€ evaluate_with_api.py     # Script principal d'Ã©valuation
â”‚   â”œâ”€â”€ evaluation_with_api.py   # Fonctions d'Ã©valuation
â”‚   â”œâ”€â”€ whisper_client.py        # Client HTTP pour l'API
â”‚   â”‚   â”œâ”€â”€ metrics.py           # Calcul des mÃ©triques
â”‚   â””â”€â”€ plot_eval_results.py     # GÃ©nÃ©ration graphiques
â”œâ”€â”€ ğŸ“Š DonnÃ©es
â”‚   â””â”€â”€ cv-corpus-21.0-delta-2025-03-14/fr/
â””â”€â”€ ğŸ“ˆ RÃ©sultats
    â”œâ”€â”€ eval_results_20clips/
    â”œâ”€â”€ eval_results_21_25/
    â””â”€â”€ eval_results_X_Y_api/
```

## ğŸš€ DÃ©marrage rapide

### 1. Installation

```bash
# Activation de l'environnement
source whisper_env/bin/activate

# VÃ©rification des dÃ©pendances
pip install -r requirements.txt
```

### 2. Test de l'architecture

```bash
# Test complet de l'architecture sÃ©parÃ©e
python test_api_architecture.py
```

### 3. Ã‰valuation complÃ¨te

```bash
# Ã‰valuation avec dÃ©marrage automatique de l'API
python evaluate_with_api.py --clips 21-25

# Avec options personnalisÃ©es
python evaluate_with_api.py \
  --clips 1-10 \
  --models tiny base large-v3 \
  --api-url http://localhost:8000 \
  --output-dir my_eval_results
```

## ğŸ¯ Utilisation

### Option 1: Ã‰valuation automatique (recommandÃ©)

```bash
# Le script gÃ¨re tout automatiquement
python evaluate_with_api.py --clips 21-25
```

**Ce qui se passe :**
1. âœ… VÃ©rifie si l'API est dÃ©jÃ  en cours
2. ğŸš€ DÃ©marre l'API si nÃ©cessaire
3. ğŸ”¬ Ã‰value tous les modÃ¨les
4. ğŸ“ˆ GÃ©nÃ¨re les graphiques
5. ğŸ›‘ ArrÃªte l'API Ã  la fin

### Option 2: API manuelle + Ã©valuations

```bash
# Terminal 1: DÃ©marrer l'API
cd api
python start_api.py --host 0.0.0.0 --port 8000

# Terminal 2: Lancer les Ã©valuations
python evaluate_with_api.py --clips 21-25 --api-url http://localhost:8000
```

### Option 3: Ã‰valuation locale (sans API)

```bash
# MÃ©thode traditionnelle sans API
python example_evaluation.py --start 21 --end 25
```

## ğŸ“¡ API Whisper

### Endpoints principaux

- `POST /transcribe_file` - Transcription d'un fichier
- `GET /models` - Liste des modÃ¨les disponibles
- `GET /cache/info` - Ã‰tat du cache
- `GET /health` - SantÃ© de l'API

### Cache intelligent

- **3 modÃ¨les maximum** en mÃ©moire simultanÃ©ment
- **Ã‰viction LRU** automatique
- **Performance optimisÃ©e** pour les Ã©valuations rÃ©pÃ©tÃ©es

### Documentation complÃ¨te

```bash
# DÃ©marrer l'API
python api/start_api.py

# AccÃ©der Ã  la doc interactive
open http://localhost:8000/docs
```

## ğŸ”¬ Ã‰valuations

### MÃ©triques calculÃ©es

- **WER** (Word Error Rate)
- **CER** (Character Error Rate)  
- **MER** (Match Error Rate)
- **WIL** (Word Information Lost)
- **BLEU** (Bilingual Evaluation Understudy)
- **ROUGE-L** (Recall-Oriented Understudy for Gisting Evaluation)

### ModÃ¨les supportÃ©s

- `tiny` (~39 MB, ~32x vitesse)
- `base` (~74 MB, ~16x vitesse)
- `small` (~244 MB, ~6x vitesse)
- `medium` (~769 MB, ~2x vitesse)
- `large` (~1550 MB, ~1x vitesse)
- `large-v3` (~1550 MB, ~1x vitesse, derniÃ¨re version)

### Formats de sortie

- **JSON** : PrÃ©dictions et mÃ©triques dÃ©taillÃ©es
- **CSV** : Comparaison tabulaire des modÃ¨les
- **PNG** : Graphiques barplots par mÃ©trique

## ğŸ”§ Configuration

### Variables d'environnement

```bash
export LANGUE=fr                    # Langue par dÃ©faut
export WHISPER_CACHE_DIR=/tmp/cache # Dossier cache modÃ¨les
```

### ParamÃ¨tres d'Ã©valuation

```bash
python evaluate_with_api.py \
  --clips 1-50 \                    # Range de clips Ã  tester
  --models tiny base large-v3 \     # ModÃ¨les Ã  Ã©valuer
  --language fr \                   # Langue de transcription
  --api-url http://localhost:8000 \ # URL de l'API
  --output-dir results_custom \     # Dossier de sortie
  --keep-api                        # Garder l'API en cours
```

## ğŸ“Š Exemples de rÃ©sultats

### Ã‰valuation sur 5 clips (clips 21-25)

| ModÃ¨le    | WER   | CER   | BLEU  | Temps moyen |
|-----------|-------|-------|-------|-------------|
| large-v3  | 0.245 | 0.089 | 0.756 | 1.8s        |
| large     | 0.267 | 0.098 | 0.728 | 1.9s        |
| medium    | 0.289 | 0.112 | 0.698 | 0.8s        |
| base      | 0.334 | 0.145 | 0.645 | 0.4s        |
| tiny      | 0.456 | 0.203 | 0.534 | 0.2s        |

### Graphiques gÃ©nÃ©rÃ©s

- `barplot_avg_wer.png` - Comparaison WER
- `barplot_avg_cer.png` - Comparaison CER  
- `barplot_avg_bleu.png` - Comparaison BLEU
- (et 5 autres mÃ©triques)

## ğŸš¨ Avantages de l'architecture API

### âœ… Avant (problÃ¨mes)

- Rechargement modÃ¨les Ã  chaque Ã©valuation
- Consommation mÃ©moire excessive
- Pas de rÃ©utilisation entre Ã©valuations
- Couplage fort modÃ¨les/Ã©valuations

### âœ… AprÃ¨s (solutions)

- **Cache intelligent** : ModÃ¨les restent en mÃ©moire
- **RÃ©utilisation** : Un modÃ¨le sert plusieurs Ã©valuations
- **SÃ©paration** : API modÃ¨les â†” Scripts Ã©valuations
- **ScalabilitÃ©** : API peut servir plusieurs clients

### Performance

```bash
# Sans API: 6 modÃ¨les Ã— 5 clips = 30 chargements
# Avec API: 6 modÃ¨les Ã— 1 chargement = 6 chargements (cache)
# Gain: ~80% de temps de chargement
```

## ğŸ” Debugging

### VÃ©rification API

```bash
# API disponible ?
curl http://localhost:8000/health

# ModÃ¨les en cache ?
curl http://localhost:8000/cache/info

# Test transcription
curl -X POST http://localhost:8000/transcribe_file \
  -F "file_path=cv-corpus-21.0-delta-2025-03-14/fr/clips/common_voice_fr_41911225.mp3" \
  -F "model_size=tiny"
```

### Logs dÃ©taillÃ©s

```bash
# API avec logs debug
python api/start_api.py --log-level debug

# Ã‰valuation avec traces
python evaluate_with_api.py --clips 21-22 --models tiny
```

## ğŸ¤ Contribuer

Le systÃ¨me est modulaire et extensible :

- **Nouvelles mÃ©triques** : Ajouter dans `evaluation/code/metrics.py`
- **Nouveaux modÃ¨les** : Modifier `whisper_models.py`
- **Nouvelles visualisations** : Ã‰tendre `plot_eval_results.py`
- **Nouveaux endpoints** : Ajouter dans `api/whisper_api.py`

## ğŸ“ Licence

Projet d'Ã©valuation des modÃ¨les Whisper pour la recherche acadÃ©mique. 